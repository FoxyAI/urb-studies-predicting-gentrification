{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Exploration\n",
    "\n",
    "This notebook in concerned with exploring the hyperparameters associated with the Random Forest regressor. It is _extremely_ computationally intensive so you should only get stuck into this if you have: a) time, and b) an interest in whether I've selected the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Needed on a Mac\n",
    "import matplotlib as mpl\n",
    "mpl.use('TkAgg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "r_state = 42\n",
    "random.seed(r_state) \n",
    "np.random.seed(r_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "print('Your scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "print('Please check it is at least 0.18.0.')\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics  \n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.externals.six import StringIO\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.feature_selection import SelectKBest \n",
    "#from sklearn.feature_selection import f_regression\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analytical = os.path.join('data','analytical')\n",
    "\n",
    "def load_status_scores(dtype):\n",
    "    status = pd.read_csv(os.path.join(analytical,dtype+'-Scores.csv.gz'), index_col=0)  # SES scores\n",
    "    \n",
    "    # Scores\n",
    "    status.drop(['RANK_01','RANK_11'], axis=1, inplace=True)\n",
    "    status.rename(columns={\n",
    "        'SES_01':'SES 2001',\n",
    "        'SES_11':'SES 2011',\n",
    "        'SES_ASC':'SES Ascent 2001-2011',\n",
    "        'SES_PR_01':'SES 2001 Percentile', # 99 = High-status\n",
    "        'SES_PR_11':'SES 2011 Percentile', # 99 = High-status\n",
    "        'SES_PR_ASC':'SES Percentile Ascent 2001-2011'\n",
    "    }, inplace=True)\n",
    "    return status\n",
    "\n",
    "def classifier_report(clf, y_true, y_hat):\n",
    "    \n",
    "    txt = ''\n",
    "    \n",
    "    # If the task is regression evaluate using regression metrics, \n",
    "    # otherwise evaluate using classification metrics\n",
    "    txt += \"R2:        {0:8.5f}\".format(metrics.r2_score(y_true, y_hat)) + \"\\n\" #  R2 - Coefficient of determination\n",
    "    txt += \"MSE:       {0:8.5f}\".format(metrics.mean_squared_error(y_true, y_hat)) + \"\\n\"  #  Mean squared error regression loss\n",
    "    txt += \"MAE:       {0:8.5f}\".format(metrics.mean_absolute_error(y_true, y_hat)) + \"\\n\"  #  Mean absolute error regression loss\n",
    "    txt += \"Expl. Var: {0:8.5f}\".format(metrics.explained_variance_score(y_true, y_hat)) + \"\\n\"  # Explained variance regression score function\n",
    "    txt += \"\\n\"\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Hyperparameters\n",
    "\n",
    "The code below is concerned with exploring the imapct that different hyperparameter settings can have on performance of the overall prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a paramter grid and explore a hyperparameter space\n",
    "# using Cross-Fold Validation...\n",
    "def explore_extr_hyper(params, x_train, y_train):\n",
    "    \n",
    "    clf = ensemble.ExtraTreesRegressor(n_jobs=-1, random_state=r_state)\n",
    "    cv  = model_selection.GridSearchCV(estimator=clf, param_grid=params, cv=4, n_jobs=2, \n",
    "                                       return_train_score=True, verbose=1, scoring='neg_mean_absolute_error') \n",
    "\n",
    "    cv.fit(x_train, y_train)\n",
    "    \n",
    "    print(\"Best score: \" + str(cv.best_score_))\n",
    "    print(\"Best parameters: \" + str(cv.best_params_))\n",
    "    \n",
    "    best_clf = cv.best_estimator_ # Extract the best estimator from the GridSearch\n",
    "    best_clf.fit(x_train, y_train)\n",
    "    y_pred  = best_clf.predict(X_test)\n",
    "\n",
    "    print(classifier_report(best_clf, y_test, y_pred))\n",
    "    return cv\n",
    "\n",
    "# Output the results of a Cross-Validation process\n",
    "# to a data frame. Currently focussed on training and\n",
    "# testing scores.\n",
    "def cv_to_df(cvr):\n",
    "    # Extract the parameters from the Cross-Validation object that \n",
    "    # we want to track in our results\n",
    "    params  = cvr.cv_results_['params']\n",
    "    trn_scr = cvr.cv_results_['mean_train_score']\n",
    "    tst_scr = cvr.cv_results_['mean_test_score']\n",
    "    trn_std = cvr.cv_results_['std_train_score']\n",
    "    tst_std = cvr.cv_results_['std_test_score']\n",
    "    rank    = cvr.cv_results_['rank_test_score']\n",
    "    \n",
    "    # Create a data frame from the numbers\n",
    "    df = pd.DataFrame.from_dict({'Training Score':trn_scr, 'Test Score':tst_scr, \n",
    "                                'Std. of Training Scores':trn_std, 'Std. of Test Scores':tst_std})\n",
    "    \n",
    "    # Add the rank of the result\n",
    "    rs = pd.Series(rank, index=df.index)\n",
    "    df['rank'] = rs\n",
    "    \n",
    "    # And now work out how many parameters there\n",
    "    # were and create the appropriate columns to\n",
    "    # add to the df. Start with named parameters...\n",
    "    n_params = cvr.cv_results_['params'][0].keys()\n",
    "    \n",
    "    # Convert these to arrays that can be assigned\n",
    "    # as a new data series to the df.\n",
    "    for p in list(n_params):\n",
    "        vals = []\n",
    "        for v in cvr.cv_results_['params']:\n",
    "            vals.append(v[p])\n",
    "        \n",
    "        # Create and assign a new series using\n",
    "        # the index from the data frame to avoid\n",
    "        # setting-with-copy warnings\n",
    "        ps = pd.Series(vals, index=df.index)\n",
    "        df[p] = ps\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can override to_use here if have already generated data above\n",
    "to_use = 'Untransformed'\n",
    "\n",
    "SES = load_status_scores(to_use)  # SES scores in 2011\n",
    "\n",
    "#  Read the transformed data\n",
    "d01_trs2 = pd.read_csv(os.path.join(analytical,to_use+'-2001-Data-Transformed_and_Scaled.csv.gz'), index_col=0)\n",
    "d11_trs2 = pd.read_csv(os.path.join(analytical,to_use+'-2011-Data-Transformed_and_Scaled.csv.gz'), index_col=0)\n",
    "\n",
    "# Data about variables used later in process\n",
    "vardb = pd.read_csv(os.path.join('data','variables.csv'), index_col=False)\n",
    "vardb.drop('Description', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the models most reliably a portion of the dataset must be kept as holdout to evaluate the classifier on independently.  The code below splits the data into training and test sets using a test size of 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    d01_trs2, SES['SES Ascent 2001-2011'], test_size=0.2, random_state=r_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_estimators\n",
    "\n",
    "This one is a beast since computations pile up as you increase the number of trees. For 400 fits on a MacBook Air I get a total running time of 5:10:40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\" : [int(x) for x in np.arange(start=20, stop=2001, step=20)]\n",
    "}\n",
    "\n",
    "start = timer()\n",
    "cv1 = explore_extr_hyper(param_grid, X_train, y_train)\n",
    "duration = timer() - start\n",
    "print(\"Execution complete in: {0:15.1f}s\".format(duration) + \" (\" + str(datetime.timedelta(seconds=duration)) + \")\")\n",
    "\n",
    "cv_to_df(cv1).to_csv(os.path.join(analytical,to_use+'-Scores-n_estimators.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_depth\n",
    "\n",
    "This appears to take approximately 36 seconds on a MacBook Air."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"max_depth\" : [int(x) for x in np.arange(start=10, stop=161, step=10)],\n",
    "}\n",
    "\n",
    "start = timer()\n",
    "cv2 = explore_extr_hyper(param_grid, X_train, y_train)\n",
    "duration = timer() - start\n",
    "print(\"Execution complete in: {0:15.1f}s\".format(duration) + \" (\" + str(datetime.timedelta(seconds=duration)) + \")\")\n",
    "\n",
    "cv_to_df(cv2).to_csv(os.path.join(analytical,to_use+'-Scores-max_depth.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_samples_leaf\n",
    "\n",
    "This is relatively quick since increasing the minimum size of terminal leaves reduces the depth of the trees substantially. It should take approximately 21 seconds on a MacBook Air."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"min_samples_leaf\" : [int(x) for x in np.arange(start=1, stop=26, step=1)],\n",
    "}\n",
    "\n",
    "start = timer()\n",
    "cv3 = explore_extr_hyper(param_grid, X_train, y_train)\n",
    "duration = timer() - start\n",
    "print(\"Execution complete in: {0:15.1f}s\".format(duration) + \" (\" + str(datetime.timedelta(seconds=duration)) + \")\")\n",
    "\n",
    "# Save results to CSV file\n",
    "cv_to_df(cv3).to_csv(os.path.join(analytical,to_use+'-Scores-min_samples_leaf.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_features & bootstrap\n",
    "\n",
    "The `max_features` applies limits to how many features each tree can employ as a share of the total number of features (1.0). Bootstrapping should not be necessary with a `k`-folds approach but in some cases can chagne the results. Running this apepars to take about 40 seconds on a MacBook Air."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"max_features\"  : [float(x) for x in np.arange(start=0.1, stop=1.01, step=0.1)], # For regression normally n_features (worth trying after shorter runs)\n",
    "    \"bootstrap\"     : [True, False]    # Not normally needed for ExtraTrees, but seems to improve performance?\n",
    "}\n",
    "\n",
    "param_grid['max_features'].append('auto')\n",
    "param_grid['max_features'].append('sqrt')\n",
    "\n",
    "start = timer()\n",
    "cv4 = explore_extr_hyper(param_grid, X_train, y_train)\n",
    "duration = timer() - start\n",
    "print(\"Execution complete in: {0:15.1f}s\".format(duration) + \" (\" + str(datetime.timedelta(seconds=duration)) + \")\")\n",
    "\n",
    "# Save results to CSV file\n",
    "cv_to_df(cv4).to_csv(os.path.join(analytical,to_use+'-Scores-max_features_and_bootstrap.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Caveat\n",
    "\n",
    "Although this exploration provides a useful overview of how the tuning of different hyperparameters can impact overall performance of the regressor, they _do not act independently of one another_. In other words: this is just exploration to get a 'feel' for the algorithm, and we will actually need to undertake a much, much, much more computationally challenging 'grid search' in [Notebook 8](08-Neighbourhood Prediction.ipynb) (or, I would suggest, [Script 8](08-Neighbourhood Prediction.py))."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ML Gentrification",
   "language": "python",
   "name": "mlgent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
